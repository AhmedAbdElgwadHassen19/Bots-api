const { GoogleGenerativeAI } = require('@google/generative-ai');
require('dotenv').config();

const genAI = new GoogleGenerativeAI(process.env.GEMINI_API_KEY);

let selectedModel = null; // âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ù…ÙˆØ¯ÙŠÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ø®ØªÙŠØ§Ø±Ù‡
let conversationContext = ""; // âœ… ØªØ®Ø²ÙŠÙ† `prompt` Ø§Ù„Ù‚Ø§Ø¯Ù… Ù…Ù† Ø§Ù„ÙØ±ÙˆÙ†Øª Ø¥Ù†Ø¯

// âœ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù…Ù† Ø§Ù„ÙØ±ÙˆÙ†Øª
const setModel = (model) => {
  if (!model) {
    console.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ù…ÙˆØ¯ÙŠÙ„ ØµØ§Ù„Ø­!");
    return;
  }
  selectedModel = model;
  console.log(`âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¥Ù„Ù‰: ${selectedModel}`);
};

// âœ… ØªØ­Ø¯ÙŠØ« `prompt` Ù…Ù† Ø§Ù„ÙØ±ÙˆÙ†Øª Ø¥Ù†Ø¯
const setPrompt = (prompt) => {
  if (!prompt || prompt.trim() === "") {
    console.error("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ‚Ø¨Ø§Ù„ Ø¨Ø±ÙˆÙ…Ø¨Øª ØµØ§Ù„Ø­!");
    return;
  }
  conversationContext = prompt.trim();
  console.log(`âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¥Ù„Ù‰: ${conversationContext}`);
};

// âœ… Ø¬Ù„Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ
const getModel = () => selectedModel;
console.log(`ðŸš€ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${selectedModel}`);

// âœ… Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø®ØªØ§Ø± Ù…Ø¹ Ø¯Ø¹Ù… inputTokens Ùˆ outputTokens
const chatCompletion = async (userMessage, inputTokens, outputTokens, retries = 3) => {
  try {
    if (!selectedModel) {
      console.error("âš ï¸ Ù„Ù… ÙŠØªÙ… ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¯ÙŠÙ„.");
      return { status: 0, response: "" };
    }

    if (!conversationContext) {
      console.warn("âš ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø¨Ø±ÙˆÙ…Ø¨Øª Ù…Ø­Ø¯Ø¯ØŒ Ø³ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¨Ø±ÙˆÙ…Ø¨Øª Ø§ÙØªØ±Ø§Ø¶ÙŠ.");
    }

    console.log(`ðŸ” Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: ${selectedModel}\n- Ø¨Ø±ÙˆÙ…Ø¨Øª: ${conversationContext}\n- Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: ${userMessage}\n- Input Tokens: ${inputTokens}\n- Output Tokens: ${outputTokens}`);

    const model = genAI.getGenerativeModel({ model: selectedModel });

    const fullPrompt = `${conversationContext}\nUser: ${userMessage}\nAssistant (ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø±Ø¯ ÙÙŠ Ø­Ø¯ÙˆØ¯ ${outputTokens} ÙƒÙ„Ù…Ø©):`;




    console.log("ðŸ“Œ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙˆÙƒÙ†Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨:", outputTokens);

    const result = await model.generateContent({
      contents: [{ parts: [{ text: fullPrompt }] }],
      generationConfig: {
        maxOutputTokens: Math.min(outputTokens * 2, 100), // Ù…Ø¶Ø§Ø¹ÙØ© Ø§Ù„Ø¹Ø¯Ø¯ Ù„Ø¶Ù…Ø§Ù† Ø§Ù„Ø¯Ù‚Ø©
        temperature: 0.3,  
        topP: 0.1 
      }
      
    });

    if (!result || !result.response || !result.response.candidates || result.response.candidates.length === 0) {
      throw new Error("âŒ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø±Ø¯ Ù…Ù† Gemini.");
    }

    const text = result.response.candidates[0].content.parts[0].text.trim();
    return { status: 1, response: text };

  } catch (error) {
    console.error("âŒ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª:", error);

    if (error.status === 503 && retries > 0) {
      console.log(`ðŸ”„ Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø¹Ø¯ 5 Ø«ÙˆØ§Ù†ÙŠ (${3 - retries}/3)...`);
      await new Promise(res => setTimeout(res, 5000));
      return chatCompletion(userMessage, inputTokens, outputTokens, retries - 1);
    }

    return { status: 0, response: "" };
  }
};

// âœ… Ù…Ù†Ø¹ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø¨Ø±ÙˆÙ…Ø¨Øª Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… Ø§Ø®ØªÙŠØ§Ø± Ù…ÙˆØ¯ÙŠÙ„
const isModelSelected = () => selectedModel !== null;

module.exports = { chatCompletion, setModel, setPrompt , getModel, isModelSelected };
